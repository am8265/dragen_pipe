The model in https://github.com/igm-team/dragen_pipe/tree/master/automated_ethnicity_relatedness/data is the same as in /nfs/goldstein/software/dragen_pipe/master/automated_ethnicity_relatedness/data

model: 37MB_markers_model.obj
markers: filtered.37MB.master.training.map
Code: model_ancestry.py, helper_functions.py (predict, train, tune etc)


The trained, tuned model is as follows:
>>> import cPickle as pickle
>>> f = open('data/37MB_markers_model.obj','rb')
>>> model = pickle.load(f)
>>> model
Pipeline(steps=[('scaling', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=6, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('nnet', MLPClassifier(activation='logistic', alpha=1e-05, batch_size='auto',
       beta_1=0.9...      solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,
       warm_start=False))])

>>> model.steps[2][1]
MLPClassifier(activation='logistic', alpha=1e-05, batch_size='auto',
       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(6,), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,
       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,
       warm_start=False)

The ith element in the coefs list represents the weight matrix corresponding to layer i
>>> model.steps[2][1].coefs_ 
[array([[-5.59176605,  0.19690849,  0.13455022,  1.7519941 , -2.31049899,
        -1.70689856],
       [-4.5654157 , -0.02212472,  1.21029041, -0.12719672,  0.25874555,
        -4.58967194],
       [-2.41409765,  0.17823914, -0.84970289, -1.20605152, -2.2492001 ,
         4.1604556 ],
       [-3.65626385,  0.04724224,  0.0864585 ,  2.22323358,  5.162491  ,
        -2.45961785],
       [-1.63150876, -0.15803884, -1.79075103,  2.2325059 , -4.80199605,
         0.76493476],
       [-1.39546601,  0.14321917, -2.07708777,  0.36625699, -6.20183385,
         1.01701626]]), array([[  6.6496609 ,   3.29630872,  -2.00797605,  -4.00751956,
         -0.78685552,  -3.60528544],
       [-10.49809428,  -2.04442901,   6.56292899,   3.96003658,
         -0.36106289,   2.1996509 ],
       [ -8.29049011,  -3.2748813 ,  -0.64824742,   6.36184682,
          1.28908282,   3.7358498 ],
       [ -3.55363252,   2.85431047,  -3.11386894,  -9.48879709,
          4.53467355,   7.64804346],
       [  4.98120554,  -2.20750991,  -3.53844739,   1.72163362,
          7.40737581,  -8.12155773],
       [  3.93943625,   3.10694028,   5.49046483,  -6.0853091 ,
         -3.5021731 ,  -3.1428167 ]])]

The ith element in the intercepts list represents the bias vector corresponding to layer i + 1.
>>> model.steps[2][1].intercepts_
[array([ 0.04324262,  1.37268838,  0.47737087, -0.00146364, -0.47800338,
        0.22920901]), array([ 3.13093887,  0.06160075,  0.13151712,  1.9346205 , -4.11007639,
       -2.04369854])]

Since I went through the code, I thought I'd write out a brief description:


Step 1: Create ped
Requires a) realn.recal.bam and analysisReady annocated vcf
         b)    markers map file filtered.37MB.master.training.map    

"create_ped_map.py" (return_pedmap function)     
1. Iterate through each chrom_pos in the marker map file:
    If site is not in vcf:
        //get coverage from bam (ignoring qual, flags)
        if bam_coverage < 10:
            //label as missing
            ped_file_entry: (0,0) 
         else:
            code = 0 
            ped_file_entry: (ref,ref)
        map_file_entry: (chrom,id_var,'0',pos) 

    else:
        if multi-allelic / indel / MNP / invalid_snv:
            // label as missing
            ped_file_entry: (0,0)
            map_file_entry: (chrom,id_var,'0',pos)  (from vcf)
        else:
            ped_file_entry = (ref,ref), (ref,alt) or (alt,alt) depending on whether genotype is (0/0, [(0/1) or (1/0)] or 1/1 )
            map_file_entry = (chrom,id_var,'0',pos)  (from vcf)

output: ped, map and geno files as follows:

ped_file: family_id    sample_id    0    0    sex    pheno    ped_file_entry1    ped_file_entry2......
map_file: (chrom,id_var,'0',pos) (1 line for each)

log the number of "missing" labels

Step 2: Use helper_functions.py predict_new_samples function and the trained model described above
1. Convert ped obtained in Step 1 -> geno encoding
    a) get positions from map file and corresponding reference allele from fasta
    b) for each ped file entry:
            if allele == ref:
                //encode 0

            if allele == 0:
                //encode 9

            else:
                keep count of allele != ref (will be 1 in case of ref-alt/ alt-ref or 2 in case of alt-alt)

    c)    count the fraction of encode = 9. 1-this_fraction is the  "genotyping_rate" in the     dragen_qc_metrics table.

    replace "encode 9" with "0" 

    using the trained model, and the encoding vector above, get the prediction probability for each ethnicity class and insert it into dragen_qc_metrics.

NOTE: The maximum number of samples for which it can predict ethnicity at a time is 10k. (hard-coded)
